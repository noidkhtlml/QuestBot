import fitz  # PyMuPDF
import re
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder

# === 1. Extrage întrebări și răspunsuri din PDF ===
def extract_questions_answers(pdf_path):
    doc = fitz.open(pdf_path)
    text = ""
    for page in doc:
        text += page.get_text()

    # Găsește întrebări și răspunsuri corecte
    pattern = r"Întrebare:(.?)Răspuns:\s([A-D])"
    matches = re.findall(pattern, text, re.DOTALL)

    questions = []
    answers = []

    for question, answer in matches:
        questions.append(question.strip())
        answers.append(answer.strip())

    return questions, answers

# === 2. Preprocesare ===
def preprocess_data(questions, answers, max_len=50):
    tokenizer = Tokenizer(oov_token="<OOV>")
    tokenizer.fit_on_texts(questions)
    sequences = tokenizer.texts_to_sequences(questions)
    padded = pad_sequences(sequences, maxlen=max_len, padding='post')

    # Convertim A/B/C/D în 0/1/2/3
    label_encoder = LabelEncoder()
    encoded_labels = label_encoder.fit_transform(answers)

    return padded, encoded_labels, tokenizer, label_encoder

# === 3. Model de clasificare ===
def build_model(input_length, vocab_size, num_classes):
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=16, input_length=input_length),
        tf.keras.layers.GlobalAveragePooling1D(),
        tf.keras.layers.Dense(32, activation='relu'),
        tf.keras.layers.Dense(num_classes, activation='softmax')
    ])
    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# === 4. Rulare completă ===
if _name_ == "_main_":
    questions, answers = extract_questions_answers("intrebari.pdf")
    X, y, tokenizer, label_encoder = preprocess_data(questions, answers)
    vocab_size = len(tokenizer.word_index) + 1
    num_classes = len(set(y))

    model = build_model(input_length=X.shape[1], vocab_size=vocab_size, num_classes=num_classes)
    model.summary()
    model.fit(X, y, epochs=10)

    # Exemplu de predicție
    test_question = ["Care e capitala Italiei? A. Berlin B. Paris C. Roma D. Londra"]
    test_seq = tokenizer.texts_to_sequences(test_question)
    test_pad = pad_sequences(test_seq, maxlen=X.shape[1], padding='post')
    pred = model.predict(test_pad)
    predicted_label = label_encoder.inverse_transform([np.argmax(pred)])
    print("Răspuns prezis:", predicted_label[0])
